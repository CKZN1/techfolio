---
layout: project
type: project
image: img/adf/servers.jpg
title: "Loading and transforming 100 million rows with Azure Data Factory"
date: 2018
published: true
labels:
  - Architecture
  - BigData
  - Oracle
  - Sql
summary: "Not your ordinary big data transformation project"
---

<img class="img-fluid" src="../img/adf/servers.jpg">

## Project Overview
One of my cherished projects involved a data transformation from Oracle to an on-premise solution, and eventually to Azure SQL. I had the opportunity to lead the team and was responsible for designing the architecture to ensure a seamless data flow.

## The Challenge
The task was far from straightforward, with the first hurdle being the volume of data we were dealing with. It was a substantial amount that really tested our infrastructure's capabilities.

## Problem-Solving
Speed. More speed...Coming up with a reliable and efficient method to handle this data, ensuring its integrity during the transformation process, was a nuanced problem that demanded a well-thought-out approach.

## Reflection
It was a scenario where technical understanding and problem-solving skills were crucial, and I appreciated every bit of the learning curve it entailed.

 
Source: <a href="https://github.com/theVacay/vacay">theVacay/vacay</a>
